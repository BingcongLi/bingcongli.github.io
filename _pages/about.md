---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Welcome, it's a pleasure to connect with you! I'm Bingcong Li, a postdoctoral researcher at ETH Zurich collaborating with [Prof. Niao He](https://odi.inf.ethz.ch/niaohe) and the [ODI group](https://odi.inf.ethz.ch/). Prior to this, I received doctoral degree from the University of Minnesota under the supervision of [Prof. Georgios B. Giannakis](https://sites.google.com/umn.edu/giannakis/home), and then I gained industry experience, dedicating a year to LLMs.


**Find me** via `bingtsongli[@]gmail.com` or `bingcong.li[@]inf.ethz.ch`.


General interests
-----------

Computation is at the heart of modern AI and the invisible engine behind the success of LLMs.
The overarching goal of my research is to **make computation in the era of LLMs efficient, accessible, and affordable across heterogeneous resources**, from clusters with thousands of GPUs to users with consumer-grade hardware. My research draws on interdisciplinary tools from *deep learning*, *optimization*, and *signal processing* to tackle challenges across multiple scales of computation. At every stage, I develop theoretically grounded methods so that the resulting systems are more explainable. I focus on three questions:

- **Foundations of computing**: Which architectures and training strategies deliver the best accuracy and efficiency?
- **Scaling computation up**: How do we sustain throughput, reliability, and cost efficiency at the scale of thousands of GPUs?
- **Personalization**: How can we empower individuals and organizations with limited resources to access and benefit from modern AI?


I enjoy cycling ðŸš´ðŸ» outside offices. I also do a bit gym training, but ocationally people tell me my triceps pushdown techniques could use some work.


Recent updates
-----------
- **12/2025.** Talked about "Scaling large efficiently for LLMs" in UC Irvine, and University of Minnesota, Twin Cities.
- **12/2025.** I will be at NeurIPS 2025 in San Diego, happy to talk!
- **10/2025.** A simplified version of [our result](https://arxiv.org/abs/2506.03133) is intergrated into ETH ODS course. 
- **10/2025.** I will give a talk at [AI Seminars](https://engineering.oregonstate.edu/EECS/research/AI-seminars) in Oregon State University.
- **10/2025.** [New paper] Stiefel manifolds, used in a general form of weight normalization, turn over-parameterization from a challenge into an advantage, provably. Check it [here](https://arxiv.org/pdf/2510.01175).
- **10/2025.** We are hosting the Efficient LLMs Fine-tuning (ELF) Track in [AI+X summit](https://www.plusx.ai/). See you in October, Zurich!
- **09/2025.** I will talk about the power of weight normalization for fine-tuning LLMs in [Quantitative Methods and Learning Research Seminar](https://www.unisg.ch/en/university/schools/school-of-economics-and-poltical-science-seps-hsg/research/quantitative-methods-and-learning/).
- **09/2025.** Three papers are accepted to NeurIPS 2025:
	- LoRA does not use allocated rank effectively. Address it with [PoLAR](https://arxiv.org/abs/2506.03133).
	- Handling symmetries for faster fine-tuning of LLMs with [RefLoRA](https://arxiv.org/abs/2505.18877).
	- Zeroth-order methods provably find flat minima. Check it out [here](https://arxiv.org/pdf/2506.05454).
- **07/2025.** I will talk about Riemannian optimization and its provable merits for fine-tuning LLMs in EUROPT 2025.
- **06/2025.** I will talk about "LoRA sugery" at [Efficient Machine Learning Reading Group](https://sites.google.com/view/efficientml).
- **05/2025.** [ICML 2025] Transfer learning provably benefits RLHF. Check out our [paper](https://arxiv.org/abs/2502.19255v3).
- **04/2025.** Talked about "Fine-tuning LLMs cost-efficiently" at Peking University.
- **01/2025.** [ICLR 2025] We prove that [initialization exponentially impacts the convergence behavior of ScaledGD](https://arxiv.org/abs/2410.18965) on LoRA type problems (i.e., linear --> quadratic rates). 
- **12/2024.** Talked about "Architecture-Aware Optimization" at ELLIS UnConference.
- **12/2024.** [ICASSP 2025] A new variant of SAM is released.
- **09/2024.** [NeurIPS 2024] We study the [implicit regularization of sharpness-aware minimization](https://arxiv.org/abs/2410.14802) (SAM) and explicify it to alleviate computational burdern of SAM. The resultant approach is useful for finetuning LLMs with LoRA.
- **05/2024.** [ICML 2024] [Memory-efficient private finetuning for LLMs](https://arxiv.org/pdf/2310.09639). We also have [a paper](https://openreview.net/pdf?id=chI7jvNkwK) at Theoretical Foundations of Foundation Models (TF2M) workshop. 
- **01/2024.** Start as a postdoc in ETH Zurich, working with Prof. Niao He. 
- **12/2023.** [ICASSP 2024] Universal 'preconditioner' for meta learning.
- **09/2023.** [NeurIPS 2023] Improving generalization by refining optimization of sharpness-aware minimization; see [here](https://arxiv.org/abs/2309.15639).
